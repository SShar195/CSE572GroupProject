{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c01c39",
   "metadata": {},
   "source": [
    "Split data into a training and testing dataset\n",
    "Due to the fact that the data is both already sorted by user and time, we will need to 'unsort' to gain a proper random sampling \n",
    "\n",
    "Steps \n",
    "    1) 'Unsort' and 'shuffle' data\n",
    "    2) develop 2 files that are accurate random samplings of data \n",
    "    3) ensure files are random samplings of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If running this in a new environment, you may need to adjust the path to the CSV file.\n",
    "# Since the file was uploaded directly, 'geophone-sensor-data.csv' might be enough.\n",
    "# For this notebook, we retain the original path.\n",
    "dataset = pd.read_csv(\"./geophone/geophone-sensor-data.csv\")\n",
    "\n",
    "dataset_sorted = dataset.sort_values(by=[\"name\", \"timestamp\"], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "og = sns.FacetGrid(dataset_sorted)\n",
    "og.map(plt.hist, 'mean', bins=20)\n",
    "\n",
    "og.set_axis_labels(\"Mean\", \"Frequency\")\n",
    "og.set_titles(col_template=\"{col_name}\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "og.figure.suptitle(\"Distribution of Mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset_sorted, test_size=0.5, random_state=42, shuffle=True)\n",
    "print(train.head())\n",
    "print('_'*40)\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = sns.FacetGrid(train)\n",
    "train_g.map(plt.hist, 'mean', bins=20)\n",
    "train_g.set_axis_labels(\"Mean\", \"Frequency\")\n",
    "train_g.set_titles(col_template=\"{col_name}\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "train_g.figure.suptitle(\"Distribution of Training Mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_g = sns.FacetGrid(test)\n",
    "test_g.map(plt.hist, 'mean', bins=20)\n",
    "\n",
    "test_g.set_axis_labels(\"Mean\", \"Frequency\")\n",
    "test_g.set_titles(col_template=\"{col_name}\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "test_g.figure.suptitle(\"Distribution of Test Mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514928bc",
   "metadata": {},
   "source": [
    "Cannot use person as a afeature due to below inconsistencies. We have to look at the data holistically. \n",
    "\n",
    "Test if there are outliers or noise in the data \n",
    "\n",
    "Binning and standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67eefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "grid = sns.FacetGrid(train, col='name', row='activity', height=2.2, aspect=1.6)\n",
    "grid.map(plt.hist, 'energy', alpha=.5, bins=20)\n",
    "grid.add_legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "grid = sns.FacetGrid(train, col='name', row='activity', height=2.2, aspect=1.6)\n",
    "grid.map(plt.scatter, 'dominant_freq', 'activity', alpha=.5)\n",
    "grid.add_legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "grid = sns.FacetGrid(train, col='name', row='activity', height=2.2, aspect=1.6)\n",
    "grid.map(plt.triplot, 'min', 'max', alpha=.5)\n",
    "grid.add_legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10028a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "grid = sns.FacetGrid(train, col='name', row='activity', height=2.2, aspect=1.6)\n",
    "grid.map(plt.ecdf, 'max', alpha=.5)\n",
    "grid.map(plt.ecdf, 'min', alpha=.5, color='red')\n",
    "grid.map(plt.ecdf, 'mean', alpha=.5, color='green')\n",
    "grid.add_legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'timestamp' column is a proper datetime object\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "grid = sns.FacetGrid(train, col='name', row='activity', height=2.2, aspect=1.6)\n",
    "grid.map(plt.scatter, 'timestamp', 'max', alpha=.5, color='blue', s=10)\n",
    "grid.map(plt.scatter, 'timestamp', 'min', alpha=.5, color='red', s=10)\n",
    "# grid.map(plt.plot('timestamp', max, label='Max Value', color='blue'))\n",
    "# grid.map(plt.plot('timestamp', min, label='Min Value', color='red'))\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "# plt.title('Max and Min Values Over Time')\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('Value')\n",
    "# plt.legend()\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24225adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'timestamp' column is a proper datetime object\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "grid = sns.FacetGrid(train, col='name', row='activity', height=2.2, aspect=1.6)\n",
    "grid.map(plt.scatter, 'timestamp', 'max', alpha=.5, color='blue', s=10)\n",
    "grid.map(plt.scatter, 'timestamp', 'min', alpha=.5, color='red', s=10)\n",
    "grid.map(plt.scatter, 'timestamp', 'mean', alpha=.5, color='green', s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'timestamp' column is a proper datetime object\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "grid = sns.FacetGrid(train, col='name', row='activity', height=2.2, aspect=1.6)\n",
    "grid.map(plt.scatter, 'timestamp', 'mean', alpha=.5, color='green', s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure the 'timestamp' column is a proper datetime object (same as original)\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "\n",
    "# 2. FILTER THE DATA FOR 'emir'\n",
    "train_emir = train[train['name'] == 'Emir']\n",
    "\n",
    "# 3. Create FacetGrid using the filtered data\n",
    "# - Removed 'col='name'' because only 'emir' remains.\n",
    "# - The row='activity' keeps the vertical separation by activity.\n",
    "grid = sns.FacetGrid(train_emir, row='activity', height=2.2, aspect=1.6)\n",
    "\n",
    "# 4. Map the scatter plot to the grid\n",
    "grid.map(plt.scatter, 'timestamp', 'mean', alpha=.5, color='green', s=10)\n",
    "\n",
    "# Optional: Add titles to the rows to show the 'activity'\n",
    "grid.set_titles(row_template='{row_name}')\n",
    "plt.show() # To display the plot, if not in a notebook environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f9c15b",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Model Training and Evaluation\n",
    "\n",
    "We now prepare the data and train three classification models to meet the project requirements. The features used are the numerical summary statistics from the geophone sensor: `mean`, `top_3_mean`, `min`, `max`, `std_dev`, `median`, `q1`, `q3`, `skewness`, `dominant_freq`, and `energy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define features and encode the target variable\n",
    "feature_cols = ['mean', 'top_3_mean', 'min', 'max', 'std_dev', 'median', 'q1', 'q3', 'skewness', 'dominant_freq', 'energy']\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply encoding to the target variable 'activity'\n",
    "y_train = le.fit_transform(train['activity'])\n",
    "y_test = le.transform(test['activity'])\n",
    "\n",
    "# Select features\n",
    "X_train = train[feature_cols]\n",
    "X_test = test[feature_cols]\n",
    "\n",
    "print(\"X_train, X_test, y_train, y_test are prepared for modeling.\")\n",
    "print(f\"Activities encoded: {list(zip(le.classes_, le.transform(le.classes_)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1a8f9",
   "metadata": {},
   "source": [
    "### A. Decision Tree Classifier (Feature Relation and Split on Attributes)\n",
    "This model helps identify the most **important features** used for classification by examining the Gini impurity/entropy reduction, which dictates how the tree **splits on attributes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(\"\\nDecision Tree Classification Report:\\n\", classification_report(y_test, y_pred_dt, target_names=le.classes_))\n",
    "\n",
    "# Feature Importance for 'Feature Relation' and 'Split on Attributes' focus\n",
    "dt_feature_importance = pd.Series(dt_model.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nDecision Tree Feature Importances (Higher score means more crucial for splitting):\\n\", dt_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d650b2a",
   "metadata": {},
   "source": [
    "### B. Logistic Regression (Regression Tests)\n",
    "Logistic Regression is utilized as a **regression test** (a simple linear classification baseline). A low performance here confirms that the activity detection requires a more complex, non-linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression is used as the 'regression test' model\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_lr, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e4e94",
   "metadata": {},
   "source": [
    "### C. Random Forest Classifier (Emphasis on Improvement and Different Feature Set)\n",
    "To achieve a better result and demonstrate **improvement**, we train a Random Forest classifier using a **restricted and high-performing subset of features** (`dominant_freq`, `energy`, `std_dev`, `skewness`, `min`, `max`) as guided by the initial feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for the new Random Forest model\n",
    "rf_feature_cols_new = ['dominant_freq', 'energy', 'std_dev', 'skewness', 'min', 'max']\n",
    "\n",
    "# Select the new feature subset\n",
    "X_train_rf_new = X_train[rf_feature_cols_new]\n",
    "X_test_rf_new = X_test[rf_feature_cols_new]\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model_new = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_new.fit(X_train_rf_new, y_train)\n",
    "y_pred_rf_new = rf_model_new.predict(X_test_rf_new)\n",
    "\n",
    "# Evaluate the new Random Forest\n",
    "rf_new_accuracy = accuracy_score(y_test, y_pred_rf_new)\n",
    "print(f\"Random Forest (New Feature Set) Accuracy: {rf_new_accuracy:.4f}\")\n",
    "print(\"\\nRandom Forest (New Feature Set) Classification Report:\\n\", classification_report(y_test, y_pred_rf_new, target_names=le.classes_))\n",
    "\n",
    "# Final comparison for the presentation EMPHASIS\n",
    "print(f\"\\nIMPROVEMENT: The new Random Forest model achieved {rf_new_accuracy:.4f} accuracy, which demonstrates a significant improvement over the Decision Tree's {dt_accuracy:.4f} baseline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}